FAISS Index Types
What is faiss.IndexLSH?
IndexLSH (Locality-Sensitive Hashing) is a FAISS index that uses hash functions to group similar vectors together. It trades accuracy for speed and memory efficiency by:

Hashing vectors into binary codes (nbits parameter)
Similar vectors get similar hash codes
Fast lookups but approximate results (not exact nearest neighbors)
Good for large-scale datasets where exact search is too slow

python# LSH with 768 hash bits
index_lsh = faiss.IndexLSH(embedding_dimension, nbits=768)

Main FAISS Index Types
1. Exact Search Indexes (100% accurate, slower)
IndexFlatL2

Brute-force L2 (Euclidean) distance
Most accurate, baseline for comparison
Good for: Small datasets (<100K vectors)

pythonindex = faiss.IndexFlatL2(dimension)
IndexFlatIP

Inner Product (dot product) similarity
Used for cosine similarity (if vectors are normalized)
Good for: Semantic search with normalized embeddings

pythonindex = faiss.IndexFlatIP(dimension)

2. Approximate Search Indexes (Fast, slightly less accurate)
IndexIVFFlat

Inverted File Index with Flat storage
Clusters vectors, searches only nearest clusters
Parameters: nlist (number of clusters), nprobe (clusters to search)
Good for: Medium datasets (100K-10M vectors)

pythonquantizer = faiss.IndexFlatL2(dimension)
index = faiss.IndexIVFFlat(quantizer, dimension, nlist=100)
index.train(training_vectors)  # Requires training!
index.add(vectors)
index.nprobe = 10  # Search 10 nearest clusters
IndexIVFPQ

IVF + Product Quantization (compression)
Compresses vectors to save memory
Parameters: nlist, m (subvectors), nbits (bits per subvector)
Good for: Large datasets (10M+ vectors), limited memory

pythonquantizer = faiss.IndexFlatL2(dimension)
index = faiss.IndexIVFPQ(quantizer, dimension, nlist=100, m=8, nbits=8)
index.train(training_vectors)
index.add(vectors)
IndexHNSWFlat

Hierarchical Navigable Small World graphs
Very fast search, high recall
Parameter: M (connections per node)
Good for: High-quality approximate search

pythonindex = faiss.IndexHNSWFlat(dimension, M=32)
index.add(vectors)
IndexLSH

Locality-Sensitive Hashing
Hash-based approximate search
Parameter: nbits (hash bits)
Good for: Very large datasets, binary similarity

pythonindex = faiss.IndexLSH(dimension, nbits=768)

3. Compressed Indexes (Save memory)
IndexPQ

Product Quantization only (no IVF)
Compresses vectors significantly
Good for: Memory-constrained environments

pythonindex = faiss.IndexPQ(dimension, m=8, nbits=8)
index.train(training_vectors)
index.add(vectors)
IndexScalarQuantizer

Scalar quantization (8-bit, 4-bit, etc.)
Simpler compression than PQ
Good for: Moderate compression needs

pythonindex = faiss.IndexScalarQuantizer(dimension, faiss.ScalarQuantizer.QT_8bit)

4. GPU Indexes (Requires GPU)
python# Convert any CPU index to GPU
gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # 0 = GPU device ID

Comparison Table
Index TypeSpeedAccuracyMemoryTraining RequiredBest ForIndexFlatL2Slow100%HighNo<100K vectors, baselineIndexFlatIPSlow100%HighNoCosine similarityIndexIVFFlatFast~95%HighYes100K-10M vectorsIndexIVFPQFast~90%LowYes10M+ vectors, limited RAMIndexHNSWFlatVery Fast~98%HighNoQuality approximate searchIndexLSHFast~85%MediumNoBinary/hash-based searchIndexPQMedium~90%Very LowYesExtreme compression

Practical Example with Multiple Indexes
pythonimport faiss
import numpy as np

dimension = 768
vectors = np.random.random((10000, dimension)).astype('float32')

# 1. Exact Search (baseline)
index_flat = faiss.IndexFlatL2(dimension)
index_flat.add(vectors)

# 2. Fast Approximate (HNSW)
index_hnsw = faiss.IndexHNSWFlat(dimension, 32)
index_hnsw.add(vectors)

# 3. Memory Efficient (IVF + PQ)
nlist = 100
quantizer = faiss.IndexFlatL2(dimension)
index_ivfpq = faiss.IndexIVFPQ(quantizer, dimension, nlist, 8, 8)
index_ivfpq.train(vectors[:5000])  # Train on subset
index_ivfpq.add(vectors)
index_ivfpq.nprobe = 10

# 4. LSH (Hash-based)
index_lsh = faiss.IndexLSH(dimension, 768)
index_lsh.add(vectors)

# Query all indexes
query = np.random.random((1, dimension)).astype('float32')
k = 5

for name, index in [("Flat", index_flat), ("HNSW", index_hnsw), 
                     ("IVFPQ", index_ivfpq), ("LSH", index_lsh)]:
    distances, indices = index.search(query, k)
    print(f"{name}: {indices[0]}")

Choosing the Right Index
Small datasets (<100K): Use IndexFlatL2 or IndexFlatIP
Medium datasets (100K-1M): Use IndexHNSWFlat or IndexIVFFlat
Large datasets (1M-100M): Use IndexIVFPQ with appropriate compression
Massive datasets (100M+): Use IndexIVFPQ with GPU acceleration
Limited memory: Use IndexPQ or IndexIVFPQ with high compression
Need exact results: Use IndexFlatL2 or IndexFlatIP
Need speed over accuracy: Use IndexLSH or IndexHNSWFlat